Apache Hadoop is an open-source framework for distributed storage and processing of large datasets. Its core components include:

1. **Hadoop Distributed File System (HDFS):**
   - **Description:** HDFS is a distributed file system designed to store and manage large volumes of data across multiple nodes in a Hadoop cluster.
   - **Key Features:**
     - Fault-tolerant: Replicates data across multiple nodes to ensure reliability.
     - Scalable: Scales horizontally by adding more nodes to the cluster.
     - High-throughput: Optimized for streaming data access patterns.

2. **Yet Another Resource Negotiator (YARN):**
   - **Description:** YARN is a resource management layer that allows different processing engines to run on Hadoop, not just MapReduce.
   - **Key Features:**
     - Resource Allocation: Manages and allocates resources (CPU, memory) to applications.
     - Multi-Tenancy: Supports multiple applications running simultaneously on the same cluster.
     - Flexibility: Enables integration with various processing frameworks like Apache Spark, Apache Flink, and others.

3. **MapReduce:**
   - **Description:** MapReduce is a programming model and processing engine for distributed data processing.
   - **Key Features:**
     - Parallel Processing: Divides tasks into smaller sub-tasks that can be processed in parallel.
     - Scalability: Scales horizontally by adding more nodes to the cluster.
     - Fault Tolerance: Recovers from node failures by redistributing tasks to healthy nodes.

4. **Hadoop Common:**
   - **Description:** Hadoop Common includes libraries, utilities, and APIs that are shared by all Hadoop modules.
   - **Key Features:**
     - Common Utilities: Provides common tools and utilities for Hadoop components.
     - Consistency: Ensures consistent behavior across different Hadoop modules.
     - Compatibility: Maintains backward compatibility and standardization.

5. **Hadoop MapReduce Libraries:**
   - **Description:** A set of libraries and tools to support the development of MapReduce applications.
   - **Key Features:**
     - Input and Output Formats: Provides formats for reading and writing data.
     - Job Control: Tools for managing and controlling MapReduce jobs.
     - Utilities: Additional utilities for MapReduce development.

These core components work together to enable the storage, processing, and analysis of large-scale data across distributed clusters. Hadoop has become a foundation for big data processing and analytics, and its ecosystem continues to evolve with additional projects and tools.
