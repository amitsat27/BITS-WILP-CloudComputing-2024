### **Complete Spark Classification Program with Iris Data and SQL Exploration**

```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col

# 1. Initialize SparkSession with options
spark = SparkSession.builder \
    .appName("CompleteSparkClassificationWithMultipleSources") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .enableHiveSupport() \
    .getOrCreate()

# Explanation:
# Initializing the SparkSession with memory configurations and enabling Hive support.
# This allows interaction with Hive tables and Spark SQL queries.
```

---

### **2. Create DataFrame from an RDD with Iris Data**

```python
# Create an RDD from a list of Iris data (for simplicity, a few rows)
iris_data_rdd = spark.sparkContext.parallelize([
    (1, 5.1, 3.5, 1.4, 0.2, "setosa"),
    (2, 4.9, 3.0, 1.4, 0.2, "setosa"),
    (3, 4.7, 3.2, 1.3, 0.2, "versicolor"),
    (4, 4.6, 3.1, 1.5, 0.2, "versicolor")
])

# Define the schema
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType
schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("sepal_length", FloatType(), True),
    StructField("sepal_width", FloatType(), True),
    StructField("petal_length", FloatType(), True),
    StructField("petal_width", FloatType(), True),
    StructField("species", StringType(), True)
])

# Create DataFrame from the RDD
iris_rdd_df = spark.createDataFrame(iris_data_rdd, schema)
iris_rdd_df.show()

# Explanation:
# The RDD contains simplified Iris data with columns like sepal length, sepal width, etc.
# We create a DataFrame from this RDD by defining a schema with appropriate column names and types.
# This DataFrame will be used for data exploration and model training.
```

---

### **3. Load Data from Hive Table with Iris Data**

```python
# Example of querying a Hive table with Iris data (replace `iris_table` with actual table name)
# Hive table should already exist with similar schema
# iris_hive_df = spark.sql("SELECT * FROM iris_table LIMIT 10")
# iris_hive_df.show()

# Explanation:
# Assuming there’s a Hive table `iris_table` with data similar to the Iris dataset, 
# we can query it using Spark SQL via `spark.sql()`. Replace `iris_table` with the actual table name.
# The table should have columns similar to `id`, `sepal_length`, `sepal_width`, etc.
```

---

### **4. SQL Data Exploration**

We’ll now use Spark SQL to explore the Iris data.

```python
# Register the DataFrame as a temporary view
iris_rdd_df.createOrReplaceTempView("iris_rdd_view")

# Perform SQL exploration - count rows
count_query = spark.sql("SELECT COUNT(*) FROM iris_rdd_view")
count_query.show()

# Perform SQL exploration - count distinct species
distinct_species_query = spark.sql("SELECT DISTINCT species FROM iris_rdd_view")
distinct_species_query.show()

# Explanation:
# After registering the DataFrame as a temporary view, we use `spark.sql()` to run SQL queries.
# `COUNT(*)` gives the number of rows in the DataFrame.
# `DISTINCT species` helps explore the unique species present in the dataset.
# These are basic SQL queries that help in understanding the dataset before further processing.
```

---

### **5. Perform DataFrame Operations (select, groupBy, filter)**

```python
# Select specific columns
selected_df = iris_rdd_df.select("sepal_length", "sepal_width", "species")
selected_df.show()

# Filter data for species 'setosa'
setosa_df = iris_rdd_df.filter(iris_rdd_df["species"] == "setosa")
setosa_df.show()

# Group by species and calculate average sepal length
grouped_df = iris_rdd_df.groupBy("species").agg({"sepal_length": "avg"})
grouped_df.show()

# Explanation:
# These operations help in further exploring the data:
# - `select()` helps choose specific columns from the dataset.
# - `filter()` filters rows based on conditions (e.g., species == 'setosa').
# - `groupBy()` allows grouping the data by a specific column (species) and applying an aggregation like averaging the sepal length.
```

---

### **6. Create Temporary Tables and Combine Data**

Let’s now combine data from the RDD DataFrame (`iris_rdd_df`) with the data from the Hive table.

```python
# Register the Hive table (assuming it's already loaded into Hive)
# iris_hive_df.createOrReplaceTempView("iris_hive_view")

# Combine data using SQL (join based on some condition)
combined_sql = spark.sql("""
    SELECT r.sepal_length, r.sepal_width, r.species, h.petal_length
    FROM iris_rdd_view r
    JOIN iris_hive_view h ON r.species = h.species
    WHERE r.sepal_length > 5.0
""")
combined_sql.show()

# Explanation:
# We join the data from the RDD DataFrame (`iris_rdd_view`) and Hive table (`iris_hive_view`) based on the `species` column.
# We filter the data by `sepal_length > 5.0` to explore only larger samples.
# This combined data can be used for further analysis or model building.
```

---

### **7. Data Preprocessing: StringIndexer and VectorAssembler**

```python
# Index the 'species' column (convert string to numeric)
indexer = StringIndexer(inputCol="species", outputCol="species_index")
processed_data = indexer.fit(combined_sql).transform(combined_sql)

# Assemble features into a single vector column
feature_columns = ["sepal_length", "sepal_width", "petal_length"]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
processed_data = assembler.transform(processed_data)

# Show the processed data
processed_data.show(5)

# Explanation:
# StringIndexer is used to convert the categorical `species` column into numeric indices (`species_index`).
# VectorAssembler combines the selected feature columns (`sepal_length`, `sepal_width`, `petal_length`) into a single vector column (`features`),
# which is needed for machine learning models.
```

---

### **8. Split Data into Training and Test Sets**

```python
# Split the data into training and test sets (80% training, 20% testing)
train_data, test_data = processed_data.randomSplit([0.8, 0.2], seed=1234)

# Explanation:
# The dataset is split into two sets: 80% for training and 20% for testing. 
# This is common practice to ensure that the model is tested on unseen data.
```

---

### **9. Train a Logistic Regression Model**

```python
# Train a Logistic Regression model
lr = LogisticRegression(featuresCol="features", labelCol="species_index")
lr_model = lr.fit(train_data)

# Explanation:
# We create a Logistic Regression model specifying the feature column (`features`) and the label column (`species_index`).
# The model is trained using the training dataset (`train_data`).
```

---

### **10. Make Predictions on Test Data**

```python
# Make predictions on the test data
predictions = lr_model.transform(test_data)

# Show predictions
predictions.select("species", "prediction", "species_index").show()

# Explanation:
# Using the trained model, we make predictions on the test data. 
# The predicted class labels are compared with the actual labels (`species_index`).
```

---

### **11. Evaluate the Model**

```python
# Evaluate the model using accuracy
evaluator = MulticlassClassificationEvaluator(labelCol="species_index", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

print(f"Test Accuracy: {accuracy:.4f}")

# Explanation:
# We use a `MulticlassClassificationEvaluator` to compute the accuracy of the model by comparing the predicted and true labels.
```

---

### **12. Show Confusion Matrix**

```python
# Show Confusion Matrix (Prediction vs Actual Labels)
predictions.groupBy("species_index", "prediction").count().show()

# Explanation:
# The confusion matrix displays how the predicted labels (species) compare to the actual labels, helping to identify 
# how well the model has classified each class.
```

---

### **13. Stop the Spark Session**

```python
# Stop the Spark session
spark.stop()

# Explanation:
# After completing the tasks, we stop the Spark session to release resources and clean up.
```

---

### **Conclusion**

This complete example now:

1. **Loads data from multiple sources** — from an **RDD** (with Iris data), **Hive table**, and **CSV** (or any other data source).
2. **Uses SQL** for data exploration (like `COUNT(*)`, `DISTINCT`) to understand the data.
3. **Performs various transformations** — like `StringIndexer`, `VectorAssembler`, and feature selection.
4. **Combines data** from multiple sources (RDD and Hive) using SQL queries and joins.
5. **Trains and evaluates a Logistic Regression model** on the processed data.

This program demonstrates how Spark can seamlessly work with multiple data sources, perform SQL queries for exploration, and build machine learning models.
