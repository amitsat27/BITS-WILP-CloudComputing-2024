### **Structured Streaming in Action**

Structured Streaming is a scalable and fault-tolerant stream processing engine in Apache Spark, built on top of the Spark SQL engine. It allows you to analyze real-time streaming data using the same APIs you use for batch processing with **DataFrames** and **Datasets**.

Structured Streaming simplifies stream processing by treating incoming data as an **unbounded table** (continuous data). Spark processes this data incrementally and updates the results in real time.

---

## **Key Components of Structured Streaming**

1. **Input Sources**:  
   Structured Streaming can consume data from various streaming sources, including:
   - **Kafka** (most common)
   - **File Sources** (e.g., JSON, CSV, Parquet)
   - **Socket Streams** (for text data)
   - **Rate Generator** (for testing)

2. **Transformation**:  
   Apply SQL-like operations (e.g., `select`, `filter`, `groupBy`) and functional transformations (`map`, `flatMap`) on the incoming stream.

3. **Output Sinks**:  
   Structured Streaming supports output to sinks such as:
   - **Console** (for testing/debugging)
   - **Kafka**
   - **Parquet, ORC, or JSON Files**
   - **Memory Sink** (for testing)
   - **Custom Sinks** (user-defined implementations)

4. **Triggers**:  
   - Specifies how often the stream data should be processed.  
   - **Available Options**:
     - **Default Trigger** (process as fast as possible)
     - **Fixed Interval** (e.g., every 10 seconds)
     - **Continuous Processing** (low-latency mode)

---

## **Structured Streaming Workflow**

### Step 1: **Set Up SparkSession**  
A `SparkSession` is required to work with Structured Streaming.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("StructuredStreamingExample") \
    .getOrCreate()
```

---

### Step 2: **Read Data from a Streaming Source**  
Structured Streaming reads real-time data as a **DataFrame**. Data sources include files, sockets, and Kafka.

#### Example: Reading from a Socket Source
```python
# Reading data from a socket
lines = spark.readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()

# Each line of text from the socket becomes a row in the DataFrame
```

---

### Step 3: **Apply Transformations**  
You can apply **SQL-like transformations** or **functional operations** to the stream.

#### Example: Word Count Transformation
```python
from pyspark.sql.functions import split, explode

# Split lines into words
words = lines.select(explode(split(lines.value, " ")).alias("word"))

# Count occurrences of each word
wordCounts = words.groupBy("word").count()
```

---

### Step 4: **Write Data to an Output Sink**  
Structured Streaming writes the processed results to a sink, updating results in real time.

#### Example: Writing to the Console Sink
```python
query = wordCounts.writeStream \
    .outputMode("complete") \  # Output all aggregated results
    .format("console") \       # Print to the console
    .trigger(processingTime="5 seconds") \  # Process every 5 seconds
    .start()

query.awaitTermination()
```

---

## **Output Modes in Structured Streaming**

1. **Append Mode**:  
   - Only new rows are added to the sink.  
   - Suitable for data that is not being updated.  
   - Example: Writing logs or streaming data to files.

2. **Update Mode**:  
   - Only updated rows (new results for existing keys) are sent to the sink.  
   - Suitable for aggregations (e.g., counts, sums).

3. **Complete Mode**:  
   - All results (entire table) are written to the sink.  
   - Suitable for **stateful aggregations** like groupBy.

---

## **Supported Operations**

Structured Streaming supports a variety of operations, such as:
- **SQL-like Operations**: `select`, `filter`, `groupBy`, `join`
- **Aggregations**: Window-based operations (tumbling, sliding windows)
- **Stateful Processing**: Managing state across batches
- **Event-Time Processing**: Processing data based on event time with **watermarking** for late data handling.

---

## **Advanced Example: Windowed Aggregation**

### Problem: Perform a **word count** over a 10-second sliding window.

```python
from pyspark.sql.functions import window

# Apply a window-based aggregation
windowedCounts = words.groupBy(
    window(words.timestamp, "10 seconds", "5 seconds"),  # Sliding window
    words.word
).count()

# Write to console
query = windowedCounts.writeStream \
    .outputMode("update") \
    .format("console") \
    .start()

query.awaitTermination()
```

---

## **Structured Streaming Features in Action**

1. **Fault-Tolerance**:  
   - Structured Streaming ensures exactly-once semantics by using **checkpointing** and **write-ahead logs** (WAL).

2. **Scalability**:  
   - Spark distributes stream processing across a cluster, ensuring scalability for large streams.

3. **Event-Time Support**:  
   - You can process data based on event time rather than arrival time, enabling accurate results even with late-arriving data.

4. **Watermarking**:  
   - Allows defining thresholds to discard late data while still performing **stateful operations**.

---

## **Summary**

Structured Streaming enables real-time data processing using the same Spark SQL APIs used for batch processing. With its fault tolerance, scalability, and support for complex operations like **windowing** and **event-time processing**, it is ideal for building modern streaming applications.

---

### **End-to-End Workflow Summary**:
1. **Read**: Stream data from a source (Kafka, files, socket).
2. **Transform**: Use SQL-like queries or functional transformations.
3. **Write**: Output results to a sink (console, Kafka, files).
4. **Trigger**: Define processing intervals (micro-batches or continuous).

Structured Streaming simplifies real-time analytics, enabling you to process and analyze streaming data **incrementally** and **fault-tolerantly**.
