### **Complete Spark Regression Program with House Pricing Data**

```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col

# 1. Initialize SparkSession with options
spark = SparkSession.builder \
    .appName("HousePricingRegression") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .enableHiveSupport() \
    .getOrCreate()

# Explanation:
# Initialize the Spark session with memory configurations and Hive support to query Hive tables.
# This enables us to load data from multiple sources, including Hive.
```

---

### **2. Create DataFrame from an RDD with House Pricing Data**

Let's assume we have some sample data on house pricing:

```python
# Sample house pricing data as an RDD
house_data_rdd = spark.sparkContext.parallelize([
    (1, 1500, 3, 10, 300000),
    (2, 1800, 4, 5, 350000),
    (3, 1200, 2, 15, 200000),
    (4, 2000, 4, 2, 450000),
    (5, 2200, 3, 8, 400000)
])

# Define the schema
from pyspark.sql.types import StructType, StructField, IntegerType, FloatType
schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("SquareFeet", IntegerType(), True),
    StructField("Bedrooms", IntegerType(), True),
    StructField("Age", IntegerType(), True),
    StructField("Price", FloatType(), True)
])

# Create DataFrame from the RDD
house_rdd_df = spark.createDataFrame(house_data_rdd, schema)
house_rdd_df.show()

# Explanation:
# The RDD contains house pricing data with features like square feet, bedrooms, age, and the target value `Price`.
# We create a DataFrame from this RDD by defining a schema with the necessary column names and types.
```

---

### **3. Load Data from Hive Table with House Pricing Data**

```python
# Example of querying a Hive table with house pricing data (replace `house_pricing_table` with actual table name)
# house_hive_df = spark.sql("SELECT * FROM house_pricing_table LIMIT 10")
# house_hive_df.show()

# Explanation:
# Assuming a Hive table `house_pricing_table` exists, we can query it using `spark.sql()`. 
# Replace `house_pricing_table` with the actual name of the table containing house pricing data.
```

---

### **4. SQL Data Exploration**

```python
# Register the DataFrame as a temporary view
house_rdd_df.createOrReplaceTempView("house_rdd_view")

# Perform SQL exploration - count rows
count_query = spark.sql("SELECT COUNT(*) FROM house_rdd_view")
count_query.show()

# Perform SQL exploration - count distinct number of bedrooms
distinct_bedrooms_query = spark.sql("SELECT DISTINCT Bedrooms FROM house_rdd_view")
distinct_bedrooms_query.show()

# Explanation:
# We register the `house_rdd_df` DataFrame as a temporary SQL view.
# SQL queries like `COUNT(*)` and `DISTINCT` help us explore the dataset and understand the distribution 
# of different features like `Bedrooms`.
```

---

### **5. Perform DataFrame Operations (select, groupBy, filter)**

```python
# Select specific columns (features and target)
selected_df = house_rdd_df.select("SquareFeet", "Bedrooms", "Age", "Price")
selected_df.show()

# Filter data where SquareFeet > 1500
filtered_df = house_rdd_df.filter(house_rdd_df["SquareFeet"] > 1500)
filtered_df.show()

# Group by the number of bedrooms and calculate average price
grouped_df = house_rdd_df.groupBy("Bedrooms").agg({"Price": "avg"})
grouped_df.show()

# Explanation:
# - `select()` allows us to select specific columns that are needed for the regression task.
# - `filter()` helps us focus on certain rows (e.g., where `SquareFeet > 1500`).
# - `groupBy()` is used to aggregate data, like calculating the average `Price` by `Bedrooms`.
```

---

### **6. Create Temporary Tables and Combine Data**

We will combine data from both the **RDD DataFrame** and **Hive table**.

```python
# Register the Hive table as a temporary view (assuming it's loaded)
# house_hive_df.createOrReplaceTempView("house_hive_view")

# Combine data from both sources using SQL join
combined_sql = spark.sql("""
    SELECT r.SquareFeet, r.Bedrooms, r.Age, r.Price, h.Price AS hive_price
    FROM house_rdd_view r
    JOIN house_hive_view h ON r.id = h.id
    WHERE r.SquareFeet > 1500
""")
combined_sql.show()

# Explanation:
# We join the data from the RDD-based DataFrame (`house_rdd_view`) and the Hive table (`house_hive_view`) 
# on the `id` column. This creates a combined dataset which can be used for further analysis or regression.
# We filter the combined data for rows where `SquareFeet > 1500`.
```

---

### **7. Data Preprocessing: VectorAssembler for Features**

```python
# Assemble features into a single vector column
feature_columns = ["SquareFeet", "Bedrooms", "Age"]
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
processed_data = assembler.transform(combined_sql)

# Show processed data
processed_data.select("features", "Price").show()

# Explanation:
# VectorAssembler is used to combine individual feature columns into a single vector column (`features`).
# This is necessary for machine learning models in Spark, which require the features to be in vector form.
```

---

### **8. Split Data into Training and Test Sets**

```python
# Split the data into training and test sets (80% training, 20% testing)
train_data, test_data = processed_data.randomSplit([0.8, 0.2], seed=1234)

# Explanation:
# We split the data into training and test datasets to evaluate the model performance.
# 80% of the data is used for training, and 20% is reserved for testing.
```

---

### **9. Train a Linear Regression Model**

```python
# Train a Linear Regression model
lr = LinearRegression(featuresCol="features", labelCol="Price")
lr_model = lr.fit(train_data)

# Explanation:
# We create a Linear Regression model specifying the feature column (`features`) and the label column (`Price`).
# The model is then trained on the training dataset (`train_data`).
```

---

### **10. Make Predictions on Test Data**

```python
# Make predictions on the test data
predictions = lr_model.transform(test_data)

# Show predictions
predictions.select("Price", "prediction").show()

# Explanation:
# Using the trained model, we make predictions on the test dataset (`test_data`).
# The model predicts the target value (`Price`), which we compare with the actual values (`Price`).
```

---

### **11. Evaluate the Model**

```python
# Evaluate the model using Root Mean Squared Error (RMSE)
evaluator = RegressionEvaluator(labelCol="Price", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)

print(f"Test RMSE: {rmse:.4f}")

# Explanation:
# The `RegressionEvaluator` computes the Root Mean Squared Error (RMSE) between the predicted and actual values.
# RMSE is a common metric for evaluating regression models, where lower values indicate better model performance.
```

---

### **12. Show Residuals and Model Performance**

```python
# Show residuals (difference between actual and predicted values)
predictions.withColumn("residual", col("Price") - col("prediction")).show()

# Explanation:
# We calculate and display the residuals (the difference between the actual and predicted values) for each row.
# This helps to identify how far off the model's predictions are from the actual target values.
```

---

### **13. Stop the Spark Session**

```python
# Stop the Spark session
spark.stop()

# Explanation:
# After completing the tasks, we stop the Spark session to release resources and clean up.
```

---

### **Conclusion**

This example demonstrates:

1. **Loading house pricing data** from multiple sources, including **RDD**, **Hive tables**, and SQL queries.
2. **SQL exploration** for basic data insights (e.g., `count(*)`, `distinct()`).
3. **Data preprocessing** using **VectorAssembler** to prepare the features for regression.
4. **Combining data** from different sources using SQL queries and joins.
5. **Training and evaluating a regression model** (Linear Regression) to predict house `Price`.
6. **Model evaluation** using RMSE and analysis of residuals.

This approach is flexible and can be adapted to any regression task, such as predicting house prices or other continuous variables.
