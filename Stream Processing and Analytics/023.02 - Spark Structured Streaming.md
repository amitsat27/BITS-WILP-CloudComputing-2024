### **Spark Structured Streaming**

**Spark Structured Streaming** is a scalable and fault-tolerant stream processing engine built on top of Apache Spark SQL. It provides a **declarative API** for defining streaming computations and supports real-time data processing with low latency.

---

### **1. Key Features**

1. **Declarative API**:
   - Uses **DataFrame** and **Dataset APIs** for defining computations, similar to batch processing.

2. **Unified Processing**:
   - Handles both **batch** and **streaming** data seamlessly in the same API.

3. **Fault Tolerance**:
   - Built-in **exactly-once** semantics with **state management** and **checkpointing**.

4. **Event Time Support**:
   - Handles **out-of-order events** with watermarks for event-time processing.

5. **Continuous Queries**:
   - Runs indefinitely, processing new data as it arrives, with output modes:
     - **Append**: Only new rows are appended.
     - **Update**: Updates rows in place.
     - **Complete**: Outputs all rows every time.

6. **Integration**:
   - Works with Kafka, HDFS, S3, and other sources/sinks.
   - Easily integrates with Spark SQL and MLlib.

---

### **2. Workflow**

1. **Data Sources**:
   - Structured Streaming reads data continuously from sources like Kafka, files, or socket connections.

2. **Processing Logic**:
   - Define the computation using DataFrame/Dataset transformations.
   - Window operations, aggregations, joins, etc.

3. **Output Sinks**:
   - Write processed data to sinks like Kafka, HDFS, MySQL, or console.

```plaintext
    Source --> Transformation --> Output Sink
```

---

### **3. Architecture**

- **Input Data**:
  - Read as an **unbounded table**.
- **Query Execution**:
  - Incrementally processes data in **micro-batches** or continuously (if supported).
- **Output Data**:
  - Writes the results to sinks.

---

### **4. Core Concepts**

#### **4.1 Streaming DataFrame/Dataset**
- A streaming DataFrame/Dataset is a continuously growing table.
- Example:
  ```scala
  val streamDF = spark.readStream.format("kafka")
                   .option("kafka.bootstrap.servers", "localhost:9092")
                   .option("subscribe", "topic")
                   .load()
  ```

#### **4.2 Event Time and Watermarking**
- **Event Time**: Time when an event occurred (e.g., in the log entry).
- **Watermarking**: Allows handling **late data** by specifying how much delay is tolerable.

Example:
```scala
val withWatermark = streamDF
  .withWatermark("eventTime", "10 minutes") // Accept events 10 mins late
  .groupBy(window($"eventTime", "5 minutes"))
  .count()
```

#### **4.3 Output Modes**
- **Append**: Add only new rows.
- **Update**: Update existing rows.
- **Complete**: Overwrite all rows in each trigger.

Example:
```scala
val query = streamDF.writeStream
  .outputMode("append")
  .format("console")
  .start()
```

---

### **5. Example Use Cases**

#### **5.1 Word Count (Socket Source)**
```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder.appName("StructuredStreaming").getOrCreate()

// Read streaming data from socket
val lines = spark.readStream
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load()

// Split lines into words
val words = lines.as[String].flatMap(_.split(" "))

// Count occurrences of each word
val wordCounts = words.groupBy("value").count()

// Write the results to the console
val query = wordCounts.writeStream
  .outputMode("complete")
  .format("console")
  .start()

query.awaitTermination()
```

#### **5.2 Kafka Source to HDFS Sink**
```scala
val kafkaStream = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .option("subscribe", "topic")
  .load()

val valueDF = kafkaStream.selectExpr("CAST(value AS STRING)")

val query = valueDF.writeStream
  .format("parquet")
  .option("path", "/path/to/hdfs")
  .option("checkpointLocation", "/path/to/checkpoint")
  .start()

query.awaitTermination()
```

---

### **6. Window Operations**

#### **Tumbling Window**
Non-overlapping fixed-sized time intervals.

```scala
val windowedCounts = streamDF
  .withWatermark("eventTime", "10 minutes")
  .groupBy(window($"eventTime", "5 minutes"))
  .count()
```

#### **Sliding Window**
Overlapping windows with a slide interval.

```scala
val slidingWindowCounts = streamDF
  .withWatermark("eventTime", "10 minutes")
  .groupBy(window($"eventTime", "5 minutes", "2 minutes"))
  .count()
```

---

### **7. Advantages Over Spark Streaming**

| **Aspect**             | **Structured Streaming**               | **Spark Streaming**                     |
|-------------------------|-----------------------------------------|------------------------------------------|
| **API**                | Declarative (DataFrame/Dataset)         | Procedural (DStreams/RDDs)              |
| **Fault Tolerance**     | Built-in                               | Requires manual checkpointing           |
| **Windowing**           | Declarative, simpler                   | Manual management                       |
| **Performance**         | Optimized via Catalyst optimizer       | Limited optimizations                   |
| **Ease of Use**         | Higher                                 | Moderate                                |

---

### **8. Challenges**

1. **Late Data Handling**:
   - Watermark configuration needs careful tuning.

2. **Resource Management**:
   - Streaming jobs require continuous resources.

3. **Stateful Processing**:
   - Managing state efficiently can be challenging for large datasets.
