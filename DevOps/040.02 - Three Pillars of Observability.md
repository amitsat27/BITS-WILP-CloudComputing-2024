### **Three Pillars of Observability**

Observability in modern systems is built on three main pillars: **Logs**, **Metrics**, and **Traces**. Together, these pillars provide comprehensive insight into the behavior and performance of applications and infrastructure, enabling teams to detect, troubleshoot, and resolve issues efficiently.

---

### **1. Logs**

Logs are records of discrete events that occur within a system. They provide context and detailed information about what happened at specific points in time.

#### **Types of Logs**
1. **System Logs**:
   - Generated by the operating system or underlying infrastructure.
   - Examples: Kernel logs, system boot logs, network logs.
   - Use Case: Troubleshooting hardware or OS-level issues.

2. **Application Logs**:
   - Generated by applications to record events like errors, user actions, or request processing.
   - Examples: API error logs, debug logs, transaction logs.
   - Use Case: Debugging application-level issues, such as a failed API call.

3. **Audit Logs**:
   - Focused on security and compliance, tracking user activities and system changes.
   - Examples: User login/logout, changes to configuration, access permissions updates.
   - Use Case: Monitoring unauthorized access or compliance violations.

#### **Characteristics**:
- Logs are timestamped to provide a chronological sequence of events.
- Typically stored in systems like **Elasticsearch**, **Splunk**, or **AWS CloudWatch Logs** for querying and analysis.

---

### **2. Metrics**

Metrics are quantitative data points collected over time, providing a numeric representation of system behavior. They are lightweight and designed for efficient aggregation and visualization.

#### **Types of Metrics**
1. **System Metrics**:
   - Measure infrastructure performance.
   - Examples: CPU usage, memory utilization, disk I/O, network latency.
   - Use Case: Monitoring resource utilization and planning capacity.

2. **Application Metrics**:
   - Track application-specific performance indicators.
   - Examples: Request latency, error rates, queue sizes, response times.
   - Use Case: Identifying slow APIs or high error rates.

3. **Business Metrics**:
   - Reflect the impact of technical metrics on business outcomes.
   - Examples: User sign-ups, revenue per hour, active users.
   - Use Case: Understanding how system performance affects business goals.

#### **Classification of Metrics**
1. **Counters**:
   - Represent cumulative values that only increase.
   - Example: Total number of requests served.
   - Use Case: Counting the total successful API calls.

2. **Gauges**:
   - Represent current values that can go up or down.
   - Example: Current memory usage or number of active sessions.
   - Use Case: Monitoring resource usage trends.

3. **Histograms**:
   - Measure the distribution of values over time.
   - Example: Request latency, with bins for <1ms, 1-5ms, 5-10ms, etc.
   - Use Case: Analyzing latency patterns.

4. **Summaries**:
   - Provide percentile metrics for aggregated data.
   - Example: 95th percentile response time of an API.
   - Use Case: Identifying outliers and ensuring SLAs.

#### **Tools**:
- **Prometheus**: Metrics collection and alerting.
- **Grafana**: Visualization and dashboarding.

---

### **3. Traces**

Traces capture the journey of a request as it flows through a distributed system. They show how different components interact to process a request.

#### **Key Concepts**
1. **Span**:
   - Represents a single operation or request within a trace.
   - Contains metadata such as start time, end time, and status.

2. **Trace**:
   - A collection of spans that represent an entire transaction.
   - Example: An HTTP request from a client interacting with multiple microservices and databases.

3. **Distributed Tracing**:
   - Tracks requests across multiple services in a microservices architecture.
   - Useful for pinpointing performance bottlenecks or failures.

#### **Benefits of Traces**:
- **End-to-End Visibility**: Helps visualize the flow of requests through the system.
- **Performance Analysis**: Identifies slow or failing services in a chain.
- **Root Cause Analysis**: Assists in determining where a failure or delay originates.

#### **Examples**:
- **Jaeger**: An open-source tracing system for monitoring and troubleshooting microservices.
- **Zipkin**: A distributed tracing system for tracking requests.

---

### **Integration of the Three Pillars**

Each pillar complements the others:
- **Logs** provide detailed event-level information for debugging.
- **Metrics** offer aggregated insights and trends.
- **Traces** show the contextual flow of requests through systems.

By combining these pillars, teams gain a complete understanding of system behavior, enabling proactive management and rapid resolution of issues.
